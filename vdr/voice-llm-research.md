# Voice Assistant LLM Research: 7-8B Models for Message Crafting

**Date:** 2026-02-02  
**Use Case:** Voice-to-text NLP layer for crafting natural Telegram messages  
**Requirements:** Fast inference (<3s), natural output, good RAG integration, runs on Ollama

---

## Executive Summary

**ðŸ† Recommended: Qwen2.5 7B**

After comprehensive analysis, **Qwen2.5 7B** offers the best balance of:
- Excellent instruction following (crucial for "craft a message" tasks)
- Fast inference (~40-60 tok/s on consumer GPU)
- Concise outputs without over-explaining
- Strong multilingual support
- 128K context window for RAG

**Runner-up:** Mistral 7B v0.3 (faster, slightly less precise following)  
**Newer Alternative:** Qwen3 8B (if thinking mode can be disabled)

---

## Model Comparison Table

| Model | Quality | Instruction Following | Speed | Context Handling | Conciseness | **Total** |
|-------|---------|----------------------|-------|------------------|-------------|-----------|
| **Qwen2.5 7B** | 9/10 | 10/10 | 8/10 | 9/10 | 9/10 | **45/50** |
| Mistral 7B v0.3 | 8/10 | 8/10 | 10/10 | 8/10 | 8/10 | **42/50** |
| Llama 3.1 8B | 9/10 | 9/10 | 7/10 | 9/10 | 7/10 | **41/50** |
| Qwen3 8B | 9/10 | 9/10 | 6/10 | 9/10 | 8/10 | **41/50** |
| Gemma 2 9B | 8/10 | 8/10 | 6/10 | 8/10 | 7/10 | **37/50** |
| Gemma 3 4B | 7/10 | 8/10 | 9/10 | 8/10 | 8/10 | **40/50** |
| OpenHermes 7B | 8/10 | 7/10 | 8/10 | 7/10 | 6/10 | **36/50** |
| Zephyr 7B | 7/10 | 7/10 | 8/10 | 7/10 | 5/10 | **34/50** |

---

## Detailed Analysis

### 1. Qwen2.5 7B â­ **RECOMMENDED**

**Ollama:** `qwen2.5:7b`

**Strengths:**
- Best-in-class instruction following among 7B models
- Trained on 18 trillion tokens
- Native 128K context window (excellent for RAG)
- Excellent at concise, task-focused outputs
- Strong multilingual support
- Tool calling support built-in

**Weaknesses:**
- Slightly slower than Mistral (~10%)
- Can occasionally be too formal

**For Voice Assistant:**
This model excels at "do exactly what I asked" tasks. When told to craft a message, it produces natural output without preamble or excessive explanations. The 128K context is overkill for this use case but ensures RAG content never gets truncated.

**Expected Speed:** 45-60 tok/s (RTX 3080), ~2-3s for typical message

---

### 2. Mistral 7B v0.3

**Ollama:** `mistral:7b`

**Strengths:**
- Fastest 7B model
- Excellent reasoning for size
- Well-tested, stable
- Tool calling support

**Weaknesses:**
- Slightly worse at precise instruction following
- Sometimes adds unwanted context

**For Voice Assistant:**
Great choice if speed is paramount. May occasionally need prompt engineering to prevent "helpful" additions to messages.

**Expected Speed:** 55-70 tok/s (RTX 3080), ~1.5-2.5s for typical message

---

### 3. Llama 3.1 8B

**Ollama:** `llama3.1:8b`

**Strengths:**
- Most capable open-source at 8B
- Excellent general knowledge
- 128K context window
- Best multilingual translation

**Weaknesses:**
- Slightly slower (8B vs 7B)
- Can be verbose without careful prompting
- Tendency toward AI-speak

**For Voice Assistant:**
Capable but may require more prompt tuning to avoid over-formal outputs. Better for complex reasoning tasks than simple message crafting.

**Expected Speed:** 40-50 tok/s (RTX 3080), ~2.5-3.5s for typical message

---

### 4. Qwen3 8B

**Ollama:** `qwen3:8b`

**Strengths:**
- Latest architecture from Alibaba
- "Thinking" capability for complex tasks
- Strong benchmarks across all categories

**Weaknesses:**
- Thinking mode adds latency (can be disabled)
- Newer = less community tuning/prompts
- Slightly larger memory footprint

**For Voice Assistant:**
Promising but the thinking feature needs to be disabled for real-time use. Worth testing once ecosystem matures. Use `/no_think` in prompts or system message.

**Expected Speed:** 35-45 tok/s with thinking, 50-60 tok/s without

---

### 5. Gemma 2 9B

**Ollama:** `gemma2:9b`

**Strengths:**
- Google quality
- Efficient architecture
- Strong on reasoning benchmarks

**Weaknesses:**
- 9B = ~20% slower than 7B
- More verbose tendencies
- 8K default context (limited RAG)

**For Voice Assistant:**
Quality is good but the size penalty and verbosity make it suboptimal for real-time message crafting.

**Expected Speed:** 35-45 tok/s (RTX 3080), ~3-4s for typical message

---

### 6. Gemma 3 4B

**Ollama:** `gemma3:4b`

**Strengths:**
- Newest architecture
- 128K context window
- Very fast (small size)
- Multimodal capabilities

**Weaknesses:**
- 4B = quality tradeoff
- May miss nuance in complex instructions

**For Voice Assistant:**
Interesting ultra-fast option. Quality might be insufficient for nuanced message crafting but worth testing for simple transformations.

**Expected Speed:** 80-100 tok/s (RTX 3080), <1.5s for typical message

---

### 7-8. Fine-tuned Models (OpenHermes, Zephyr)

**Ollama:** `openhermes:7b`, `zephyr:7b`

**For Voice Assistant: NOT RECOMMENDED**

These chat-optimized fine-tunes are designed for conversational assistance, which means they:
- Add preambles ("Sure, I'd be happy to help!")
- Over-explain ("Here's the message I've crafted for you:")
- Use AI-speak patterns
- Are harder to constrain to exact output

For voice assistant message crafting, you want the opposite: minimal framing, direct output.

---

## Test Results

### Test Prompt:
```
You help craft Telegram messages. Take the user's spoken input and create a natural message as if they typed it.

User said: "Tell Kira that I will check the IAM website tomorrow and also remind her about the ZenithCred deadline"

Context: Recent discussion about IAM website rebrand (deadline Feb 4) and ZenithCred investment round.

Craft the message:
```

### Expected Good Output:
```
Hey Kira! I'll check out the IAM website tomorrow. Also, don't forget about the ZenithCred deadline!
```

### Model Output Patterns:

| Model | Output Quality | Issues |
|-------|----------------|--------|
| Qwen2.5 7B | âœ… Excellent | Direct, natural tone |
| Mistral 7B | âœ… Good | Occasionally adds context |
| Llama 3.1 8B | âš ï¸ Okay | Sometimes too formal |
| Qwen3 8B | âœ… Good | May add reasoning tokens |
| Gemma 2 9B | âš ï¸ Okay | Can be verbose |
| OpenHermes 7B | âŒ Poor | Adds preamble |
| Zephyr 7B | âŒ Poor | Over-explains |

---

## Recommended Configuration

### Pull Command
```bash
ollama pull qwen2.5:7b
```

### Optimal Settings

**For Ollama API:**
```json
{
  "model": "qwen2.5:7b",
  "options": {
    "temperature": 0.3,
    "top_p": 0.9,
    "top_k": 40,
    "num_predict": 150,
    "stop": ["\n\n", "---"]
  }
}
```

**Rationale:**
- `temperature: 0.3` â€” Low for consistent, predictable output
- `top_p: 0.9` â€” Slight diversity for natural variation
- `num_predict: 150` â€” Messages shouldn't be long; caps runaway generation
- `stop` tokens â€” Prevent model from continuing past the message

### System Prompt
```
You are a message assistant. Transform spoken input into natural Telegram messages.

Rules:
- Output ONLY the message, no explanation
- Match the user's casual tone
- Keep it concise
- Use context naturally without over-referencing
- Sound like a human typing quickly
```

---

## Alternative Recommendations

### If Speed is Critical (sub-2s required):
```bash
ollama pull mistral:7b
# Or for ultra-fast:
ollama pull gemma3:4b
```

### If Quality is More Important:
```bash
ollama pull llama3.1:8b
# Use with slightly higher temperature (0.4-0.5)
```

### For Future Consideration:
- **Qwen3 8B** â€” Watch for `/no_think` mode optimization
- **Ministral 8B** â€” Mistral's newer 8B model when available on Ollama
- **Gemma 3 4B-it-qat** â€” Quantization-aware trained version

---

## Hardware Considerations

| GPU VRAM | Recommended Model | Expected Speed |
|----------|-------------------|----------------|
| 8GB | qwen2.5:7b-q4_K_M | 30-40 tok/s |
| 12GB | qwen2.5:7b | 45-60 tok/s |
| 16GB+ | qwen2.5:7b (FP16) | 50-70 tok/s |
| CPU-only | mistral:7b-q4_0 | 5-10 tok/s âš ï¸ |

For voice assistant use, CPU-only is likely too slow. Recommend GPU with 8GB+ VRAM minimum.

---

## Quick Start

```bash
# 1. Pull the model
ollama pull qwen2.5:7b

# 2. Test it
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "Transform to Telegram message: Tell mom Ill be late for dinner around 8pm",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "num_predict": 100
  }
}'

# 3. Integrate with your voice pipeline
```

---

## Conclusion

**Qwen2.5 7B** is the clear winner for voice assistant message crafting:
- Fast enough for real-time use (~2-3s)
- Best instruction following in its class
- Naturally concise without prompt hacking
- Excellent context handling for RAG
- Well-supported in Ollama ecosystem

Start with Qwen2.5 7B, use the recommended settings, and tune the system prompt based on your specific user's communication style.
